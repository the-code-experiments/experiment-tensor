{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Retore TF models\n",
    "\n",
    "what could potentially be saved when you are doing machine learning?\n",
    "\n",
    "1. You can save the architecture of the model and the learned weights associated with it.\n",
    "2. You might want to save some training characteristics like the loss and accuracy of the model while training or even the whole training architecture.\n",
    "3. You might want to save hyperparameters and other operations to restart training later or replicate a result.\n",
    "\n",
    "## File architecture\n",
    "\n",
    "In TF, when you save your data the usual way, you end up with different type of files:\n",
    "\n",
    "\n",
    "![Save Model Structure](./_assets_/save_model.png)\n",
    "\n",
    "\n",
    "1. A `checkpoint` file: The checkpoint file is just a bookkeeping file that you can use in combination of high-level helper for loading different time saved chkp files.\n",
    "\n",
    "2. Some `data` files: The `.data` files hold the data (weights) itself (this one is usually quite big in size). There can be many data files because they can be sharded and/or created on multiple timesteps while training. Data files are a lot heavier than the meta files.\n",
    "\n",
    "3. A `meta` file: The `.meta` file holds the compressed Protobufs graph of your model and all the metadata associated (collections, learning rate, operations, etc.)\n",
    "\n",
    "4. An `index` file: The .index file holds an immutable key-value table linking a serialised tensor name and where to find its data in the chkp.data files. The index file is very light as expected since itâ€™s just a key-value table.\n",
    "\n",
    "5. If you use `TensorBoard`, an `event` file: The events file store everything you need to visualise your model and all the data measured while you were training using summaries. This has nothing to do with saving/restoring your models itself.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "* The weights filename is as follow: `<prefix>-<global_step>.data-<shard_index>-of-<number_of_shards>`.\n",
    "* The model has been saved 2 data files are a lot heavier than the meta files which is to be expected as they are containing the weights of our model.\n",
    "\n",
    "\n",
    "### Protocal Buffers\n",
    "\n",
    "* Protocol Buffers often abbreviated Protobufs is the format used by TF to store and transfer data efficiently.\n",
    "\n",
    "You can use Protobufs as:\n",
    "\n",
    "* An uncompressed, human-friendly, text format with the extension `.pbtxt`\n",
    "* A compressed, machine friendly, binary format with the extension `.pb` or `no extension at all`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1208 12:28:23.997215 140734901114304 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "<tensorflow.python.framework.ops.Graph object at 0x130525710>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Currently, we are in the default graph scope\n",
    "# Let's design some variables\n",
    "v1 = tf.Variable(1. , name=\"v1\")\n",
    "v2 = tf.Variable(2. , name=\"v2\")\n",
    "\n",
    "# Let's design an operation\n",
    "a = tf.add(v1, v2)\n",
    "\n",
    "# We can check easily that we are indeed in the default graph\n",
    "print(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a Saver object\n",
    "# By default, the Saver handles every Variables related to the default graph\n",
    "all_saver = tf.train.Saver()\n",
    "\n",
    "# But you can precise which vars you want to save (as a list) and under which name (with a dict)\n",
    "v2_saver = tf.train.Saver({\"v2\": v2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_dir = './_save_model_'\n",
    "\n",
    "# By default the Session handles the default graph and all its included variables\n",
    "with tf.Session() as sess:\n",
    "  # Init v1 and v2   \n",
    "  sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "  # Now v1 holds the value 1.0 and v2 holds the value 2.0\n",
    "  # and we can save them\n",
    "  all_saver.save(sess, save_model_dir + '/data-all')\n",
    "\n",
    "  # or saves only v2\n",
    "  v2_saver.save(sess, save_model_dir + '/data-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
