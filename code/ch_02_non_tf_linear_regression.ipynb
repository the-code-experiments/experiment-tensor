{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "* Linear regression is a linear model.\n",
    "* A model that assumes a linear relationship between the input variable (x) and the single output variable (y).\n",
    "* (y) can be calculated from a linear combination of the input variable (x).\n",
    "* When there is a single input variable (x), the method is referred to as **simple linear regression**.\n",
    "* When there are multiple input variable, its often refers to as **multiple linear regression**.\n",
    "* Input (x) and output (y) are numeric.\n",
    "\n",
    "In Math, the linear regression line has an equation of the form:\n",
    "\n",
    "```\n",
    "y = xw + b\n",
    "```\n",
    "\n",
    "where,\n",
    "\n",
    "* x is the explanatory variable\n",
    "* y is the dependent variable\n",
    "* w is the slope of the line\n",
    "* b is the intercept (the value of y when x = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Optional, not required for model algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random input data to train on\n",
    "\n",
    "1. Draw samples from a [uniform distribution](http://mathworld.wolfram.com/UniformDistribution.html) using [Numpy's Random Uniform method](https://het.as.utexas.edu/HET/Software/Numpy/reference/generated/numpy.random.uniform.html).\n",
    "\n",
    "2. In uniform distribution, `size = number of observations (n) * number of variables (k)`\n",
    "\n",
    "```\n",
    "input = n * k = 1000 * 1 [here k=1, as we are consider only 1 input (x)]\n",
    "```\n",
    "\n",
    "We have create a input data like `1000 * 1` matrix, which means `1` variables problem with `1000` observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = 1000\n",
    "\n",
    "\"\"\"\n",
    "1. Here, we are creating 2 input variables (x) and (z) problem with 1000 observations.\n",
    "\"\"\"\n",
    "x = np.random.uniform(low=-10, high=10, size=(observations, 1))\n",
    "z = np.random.uniform(low=-10, high=10, size=(observations, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In our example we have 2 input variables and 1000 observations, thus we need `1000 * 2` matrix.\n",
    "* We will combine samples (x) and (z) into `1` martix, by using `column_stack` function which takes a sequence of 1D-array and stacks them into a single 2D-array, which results into a matrix of `1000 * 2`.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Dimension are important in linear algebram as we can only multiple matrix that are compatible.\n",
    "* We want to make sure we can multiple (x) times (w), since our linear model relies on that.\n",
    "* We have created an input data `1000 * 2`, means `2` variables problems with `1000` observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "inputs = np.column_stack((x, z))\n",
    "\n",
    "# Lets know the shape of the input to verify the results\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the targets we will aim at\n",
    "\n",
    "In supervized learning, below are the elements of the model:\n",
    "\n",
    "1. Inputs\n",
    "2. Weights\n",
    "3. Biases\n",
    "4. Outputs\n",
    "5. Targets\n",
    "\n",
    "**Notes**\n",
    "\n",
    "* 2 major parameters are `inputs` and `targets`.\n",
    "* We also need to generate targets.\n",
    "* Weights, Biases and Outputs varies based on the algorithms.\n",
    "\n",
    "Consider a targets = `f(x, z) = 2*x - 3*z + 5 + noise`. (*Completely arbitrarily chosen, We can try different functions as well.*)\n",
    "\n",
    "Conceptually, the algorithm must learn this is the function and the Weights are `2` and `-3`\n",
    "and the Bias is `5`.\n",
    "\n",
    "We are introducing noice will make data bit random. Real data always contents noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "noise = np.random.uniform(low=-1, high=1, size=(observations, 1))\n",
    "\n",
    "targets = 2*x - 3*z + 5 + noise\n",
    "# targets = 13*x - 7*z - 12 + noise # Try diff formula values\n",
    "\n",
    "# Lets verify the shape of target after the x and z are applied\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize variables\n",
    "\n",
    "* Lets review the linear model, `y = xw + b`.\n",
    "* The model will try to find the value for `w` and `b` such that the output (y) are closer to the targets (t).\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Our initial weights and biases will be picked randomly from the interval `[-0.1, 0.1]` and we will generate them using uniform function as we did it before.\n",
    "\n",
    "2. Set a learning rate. This is called `etas`. Different etas affect the speed of optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03759142]\n",
      " [ 0.0336922 ]]\n",
      "[-0.05975424]\n"
     ]
    }
   ],
   "source": [
    "init_range = 0.1\n",
    "\n",
    "# As we have 2 input variables, we have 2 weights and we need to get the sample of size (2 * 1)\n",
    "weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))\n",
    "print(weights)\n",
    "\n",
    "# Biases is a scale (1 * 1), if there are \"n\" outputs, there will \"n\" biases.\n",
    "biases = np.random.uniform(low=-init_range, high=init_range, size=1)\n",
    "print(biases)\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.02 # etas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Since this is an iterative problem, we need an iteration which has below steps:\n",
    "\n",
    "1. Calculate outputs\n",
    "2. Campare outputs to targets through the loss function (L2-norm loss function)\n",
    "3. Adjust weights and biases\n",
    "4. Repeat steps from (1) to (3) util we get the better match\n",
    "\n",
    "**References**\n",
    "\n",
    "* [Different between L1-norm vs L2-norm loss function](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/).\n",
    "* [On Keggle L1-norm vs L2-norm loss functions](https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217.48043773578937\n",
      "39.83416309188944\n",
      "15.89605111940113\n",
      "12.244817835902243\n",
      "11.350601483865576\n",
      "10.851778384099234\n",
      "10.42302825500306\n",
      "10.018297266405163\n",
      "9.630510805744201\n",
      "9.258126108944456\n",
      "8.900411174060658\n",
      "8.556770685056035\n",
      "8.226648347303888\n",
      "7.909512014497566\n",
      "7.604850803087149\n",
      "7.312173974141362\n",
      "7.0310100998325\n",
      "6.760906297798266\n",
      "6.501427500745809\n",
      "6.252155755536424\n",
      "6.01268954995159\n",
      "5.7826431659578885\n",
      "5.5616460584147465\n",
      "5.3493422582221175\n",
      "5.1453897989464545\n",
      "4.949460166001361\n",
      "4.761237767495666\n",
      "4.580419425896653\n",
      "4.406713889689551\n",
      "4.23984136424681\n",
      "4.079533061151396\n",
      "3.925530765248295\n",
      "3.777586418726767\n",
      "3.635461721563463\n",
      "3.4989277476828056\n",
      "3.3677645762163806\n",
      "3.241760937267382\n",
      "3.1207138716095195\n",
      "3.00442840377228\n",
      "2.892717227985944\n",
      "2.785400406480503\n",
      "2.6823050796525214\n",
      "2.583265187633079\n",
      "2.4881212028083426\n",
      "2.396719872861896\n",
      "2.3089139739249473\n",
      "2.2245620734368146\n",
      "2.143528302333702\n",
      "2.065682136198826\n",
      "1.9908981850213885\n",
      "1.919055991225752\n",
      "1.8500398356454728\n",
      "1.7837385511297086\n",
      "1.7200453434817218\n",
      "1.658857619441098\n",
      "1.6000768214325727\n",
      "1.5436082688152963\n",
      "1.4893610053768604\n",
      "1.4372476528263878\n",
      "1.3871842700507386\n",
      "1.3390902179071205\n",
      "1.2928880293343181\n",
      "1.2485032845733122\n",
      "1.2058644912963383\n",
      "1.164902969451251\n",
      "1.1255527406357553\n",
      "1.0877504218232856\n",
      "1.051435123269372\n",
      "1.0165483504340287\n",
      "0.983033909762199\n",
      "0.9508378181704993\n",
      "0.9199082160944465\n",
      "0.8901952839561386\n",
      "0.8616511619178128\n",
      "0.8342298727920465\n",
      "0.8078872479844121\n",
      "0.7825808563493221\n",
      "0.7582699358444273\n",
      "0.7349153278735405\n",
      "0.7124794142122709\n",
      "0.6909260564148102\n",
      "0.6702205376042566\n",
      "0.6503295065527199\n",
      "0.6312209239611292\n",
      "0.6128640108522211\n",
      "0.5952291989935804\n",
      "0.5782880832708752\n",
      "0.5620133759345763\n",
      "0.5463788626464565\n",
      "0.5313593602550869\n",
      "0.5169306762322997\n",
      "0.5030695697052899\n",
      "0.48975371402159285\n",
      "0.4769616607866263\n",
      "0.4646728053158836\n",
      "0.4528673534461237\n",
      "0.44152628965210144\n",
      "0.43063134641748035\n",
      "0.42016497481059706\n",
      "0.4101103162176739\n"
     ]
    }
   ],
   "source": [
    "# Follow the Linear model equation (Y = xw + b)\n",
    "\n",
    "for i in range (100):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the outputs by given weights and biases\n",
    "    \"\"\"\n",
    "    outputs = np.dot(inputs, weights) + biases\n",
    "    \n",
    "    \"\"\"\n",
    "    Record the difference between outputs and targets\n",
    "    \"\"\"\n",
    "    deltas = outputs - targets\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the loss function that compares the outputs to the targets\n",
    "    Use L2-norm loss/2 function to calculate the loss.\n",
    "    Below will give Loss/Obseraction = Mean (average) loss\n",
    "    \"\"\"\n",
    "    loss = np.sum(deltas ** 2) / 2 / observations\n",
    "    \n",
    "    \"\"\"\n",
    "    Print the loss for analysis to see at each step, whether the loss is decreasing\n",
    "    \"\"\"\n",
    "    print(loss)\n",
    "    \n",
    "    deltas_scaled = deltas / observations\n",
    "    \n",
    "    \"\"\"\n",
    "    Updates the weights and biases by following the Gradient descent methodology\n",
    "    \"\"\"\n",
    "    weights = weights - learning_rate * np.dot(inputs.T, deltas_scaled)\n",
    "    biases = biases - learning_rate * np.sum(deltas_scaled) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print weights and biases and see if we have worked correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.99640501]\n",
      " [-2.98950591]] [4.32964086]\n"
     ]
    }
   ],
   "source": [
    "print(weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot last outputs vs targets\n",
    "\n",
    "Since they are the last ones at the end of the training, they represent the final model accuracy.\n",
    "The closed this plot is to a 45 degree line, the closer target and output values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(outputs, targets)\n",
    "plt.xlabel(\"Outputs\")\n",
    "plt.ylabel(\"Targets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
