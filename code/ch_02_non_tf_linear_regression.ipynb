{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "* Linear regression is a linear model.\n",
    "* A model that assumes a linear relationship between the input variable (x) and the single output variable (y).\n",
    "* (y) can be calculated from a linear combination of the input variable (x).\n",
    "* When there is a single input variable (x), the method is referred to as **simple linear regression**.\n",
    "* When there are multiple input variable, its often refers to as **multiple linear regression**.\n",
    "* Input (x) and output (y) are numeric.\n",
    "\n",
    "In Math, the linear regression line has an equation of the form:\n",
    "\n",
    "```\n",
    "y = xw + b\n",
    "```\n",
    "\n",
    "where,\n",
    "\n",
    "* x is the explanatory variable\n",
    "* y is the dependent variable\n",
    "* w is the slope of the line\n",
    "* b is the intercept (the value of y when x = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Optional, not required for model algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random input data to train on\n",
    "\n",
    "1. Draw samples from a [uniform distribution](http://mathworld.wolfram.com/UniformDistribution.html) using [Numpy's Random Uniform method](https://het.as.utexas.edu/HET/Software/Numpy/reference/generated/numpy.random.uniform.html).\n",
    "\n",
    "2. In uniform distribution, `size = number of observations (n) * number of variables (k)`\n",
    "\n",
    "```\n",
    "input = n * k = 1000 * 1 [here k=1, as we are consider only 1 input (x)]\n",
    "```\n",
    "\n",
    "We have create a input data like `1000 * 1` matrix, which means `1` variables problem with `1000` observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = 1000\n",
    "\n",
    "\"\"\"\n",
    "1. Here, we are creating 2 input variables (x) and (z) problem with 1000 observations.\n",
    "\"\"\"\n",
    "x = np.random.uniform(low=-10, high=10, size=(observations, 1))\n",
    "z = np.random.uniform(low=-10, high=10, size=(observations, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In our example we have 2 input variables and 1000 observations, thus we need `1000 * 2` matrix.\n",
    "* We will combine samples (x) and (z) into `1` martix, by using `column_stack` function which takes a sequence of 1D-array and stacks them into a single 2D-array, which results into a matrix of `1000 * 2`.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Dimension are important in linear algebram as we can only multiple matrix that are compatible.\n",
    "* We want to make sure we can multiple (x) times (w), since our linear model relies on that.\n",
    "* We have created an input data `1000 * 2`, means `2` variables problems with `1000` observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "inputs = np.column_stack((x, z))\n",
    "\n",
    "# Lets know the shape of the input to verify the results\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the targets we will aim at\n",
    "\n",
    "In supervized learning, below are the elements of the model:\n",
    "\n",
    "1. Inputs\n",
    "2. Weights\n",
    "3. Biases\n",
    "4. Outputs\n",
    "5. Targets\n",
    "\n",
    "**Notes**\n",
    "\n",
    "* 2 major parameters are `inputs` and `targets`.\n",
    "* We also need to generate targets.\n",
    "* Weights, Biases and Outputs varies based on the algorithms.\n",
    "\n",
    "Consider a targets = `f(x, z) = 2*x - 3*z + 5 + noise`. (*Completely arbitrarily chosen, We can try different functions as well.*)\n",
    "\n",
    "Conceptually, the algorithm must learn this is the function and the Weights are `2` and `-3`\n",
    "and the Bias is `5`.\n",
    "\n",
    "We are introducing noice will make data bit random. Real data always contents noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "noise = np.random.uniform(low=-1, high=1, size=(observations, 1))\n",
    "\n",
    "targets = 2*x - 3*z + 5 + noise\n",
    "# targets = 13*x - 7*z - 12 + noise # Try diff formula values\n",
    "\n",
    "# Lets verify the shape of target after the x and z are applied\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize variables\n",
    "\n",
    "* Lets review the linear model, `y = xw + b`.\n",
    "* The model will try to find the value for `w` and `b` such that the output (y) are closer to the targets (t).\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Our initial weights and biases will be picked randomly from the interval `[-0.1, 0.1]` and we will generate them using uniform function as we did it before.\n",
    "\n",
    "2. Set a learning rate. This is called `etas`. Different etas affect the speed of optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04523302]\n",
      " [ 0.06382485]]\n",
      "[0.00107571]\n"
     ]
    }
   ],
   "source": [
    "init_range = 0.1\n",
    "\n",
    "# As we have 2 input variables, we have 2 weights and we need to get the sample of size (2 * 1)\n",
    "weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))\n",
    "print(weights)\n",
    "\n",
    "# Biases is a scale (1 * 1), if there are \"n\" outputs, there will \"n\" biases.\n",
    "biases = np.random.uniform(low=-init_range, high=init_range, size=1)\n",
    "print(biases)\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.02 # etas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Since this is an iterative problem, we need an iteration which has below steps:\n",
    "\n",
    "1. Calculate outputs\n",
    "2. Campare outputs to targets through the loss function (L2-norm loss function)\n",
    "3. Adjust weights and biases\n",
    "4. Repeat steps from (1) to (3) util we get the better match\n",
    "\n",
    "**References**\n",
    "\n",
    "* [Different between L1-norm vs L2-norm loss function](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/).\n",
    "* [On Keggle L1-norm vs L2-norm loss functions](https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251.2195686759099\n",
      "32.89392579554103\n",
      "13.410856306291171\n",
      "11.268394746942239\n",
      "10.680904661759728\n",
      "10.251071968831951\n",
      "9.851100230910404\n",
      "9.46824989137978\n",
      "9.10067436859692\n",
      "8.747641963306181\n",
      "8.408562712070577\n",
      "8.08288325973516\n",
      "7.770073813956333\n",
      "7.469625748842353\n",
      "7.181050570785504\n",
      "6.903879094909206\n",
      "6.63766067795568\n",
      "6.38196248496429\n",
      "6.136368785444924\n",
      "5.900480277442253\n",
      "5.6739134383228595\n",
      "5.456299901219239\n",
      "5.247285856115341\n",
      "5.046531474599584\n",
      "4.853710357350068\n",
      "4.668509003453666\n",
      "4.490626300696197\n",
      "4.3197730359948885\n",
      "4.15567142517725\n",
      "3.9980546613417935\n",
      "3.8466664810662823\n",
      "3.7012607477582686\n",
      "3.5616010514704657\n",
      "3.4274603245303195\n",
      "3.298620472358869\n",
      "3.1748720188786046\n",
      "3.056013765933866\n",
      "2.941852466169996\n",
      "2.832202508839432\n",
      "2.7268856180238683\n",
      "2.625730562781852\n",
      "2.5285728787505795\n",
      "2.4352546007491833\n",
      "2.345624005948848\n",
      "2.2595353671921283\n",
      "2.1768487160603875\n",
      "2.097429615304217\n",
      "2.0211489402667193\n",
      "1.9478826689444013\n",
      "1.8775116803442515\n",
      "1.8099215608092052\n",
      "1.7450024179971075\n",
      "1.6826487022106966\n",
      "1.6227590347881677\n",
      "1.565236043275238\n",
      "1.5099862031107947\n",
      "1.4569196855686641\n",
      "1.4059502117083302\n",
      "1.3569949120971079\n",
      "1.3099741920757129\n",
      "1.2648116023481721\n",
      "1.2214337146856686\n",
      "1.1797700025422233\n",
      "1.1397527263881222\n",
      "1.101316823574662\n",
      "1.06439980255112\n",
      "1.0289416412620003\n",
      "0.9948846895593276\n",
      "0.9621735754713581\n",
      "0.9307551151752673\n",
      "0.9005782265275024\n",
      "0.8715938460111585\n",
      "0.8437548489653661\n",
      "0.8170159729670151\n",
      "0.791333744240199\n",
      "0.7666664069737694\n",
      "0.7429738554320735\n",
      "0.7202175687484721\n",
      "0.6983605482956552\n",
      "0.677367257530906\n",
      "0.6572035642185133\n",
      "0.6378366849353975\n",
      "0.6192351317697258\n",
      "0.6013686611258472\n",
      "0.5842082245523227\n",
      "0.5677259215130902\n",
      "0.551894954024988\n",
      "0.5366895830878807\n",
      "0.5220850868365436\n",
      "0.5080577203462726\n",
      "0.4945846770268659\n",
      "0.4816440515422094\n",
      "0.4692148041951815\n",
      "0.4572767267199641\n",
      "0.4458104094261513\n",
      "0.4347972096412342\n",
      "0.4242192214001498\n",
      "0.4140592463326168\n",
      "0.404300765700924\n",
      "0.3949279135427107\n"
     ]
    }
   ],
   "source": [
    "# Follow the Linear model equation (Y = xw + b)\n",
    "\n",
    "for i in range (100):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the outputs by given weights and biases\n",
    "    \"\"\"\n",
    "    outputs = np.dot(inputs, weights) + biases\n",
    "    \n",
    "    \"\"\"\n",
    "    Record the difference between outputs and targets\n",
    "    \"\"\"\n",
    "    deltas = outputs - targets\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the loss function that compares the outputs to the targets\n",
    "    Use L2-norm loss/2 function to calculate the loss.\n",
    "    Below will give Loss/Obseraction = Mean (average) loss\n",
    "    \"\"\"\n",
    "    loss = np.sum(deltas ** 2) / 2 / observations\n",
    "    \n",
    "    \"\"\"\n",
    "    Print the loss for analysis to see at each step, whether the loss is decreasing\n",
    "    \"\"\"\n",
    "    print(loss)\n",
    "    \n",
    "    deltas_scaled = deltas / observations\n",
    "    \n",
    "    \"\"\"\n",
    "    Updates the weights and biases by following the Gradient descent methodology\n",
    "    \"\"\"\n",
    "    weights = weights - learning_rate * np.dot(inputs.T, deltas_scaled)\n",
    "    biases = biases - learning_rate * np.sum(deltas_scaled) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print weights and biases and see if we have worked correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.00504114]\n",
      " [-2.99232282]] [4.31250879]\n"
     ]
    }
   ],
   "source": [
    "print(weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot last outputs vs targets\n",
    "\n",
    "Since they are the last ones at the end of the training, they represent the final model accuracy.\n",
    "The closed this plot is to a 45 degree line, the closer target and output values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(outputs, targets)\n",
    "plt.xlabel(\"Outputs\")\n",
    "plt.ylabel(\"Targets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
